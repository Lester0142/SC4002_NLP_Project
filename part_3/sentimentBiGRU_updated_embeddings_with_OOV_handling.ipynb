{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec_model = KeyedVectors.load(r\"../word2vec-google-news-300.model\")\n",
    "print(len(word2vec_model))\n",
    "# print(word2vec_model.key_to_index[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15583\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "# this code is taken from online and slightly modified\n",
    "\n",
    "with open(r\"../vocab.json\") as f:\n",
    "    dict = json.load(f)\n",
    "    word_set = set(dict.keys())\n",
    "vocab = set(word2vec_model.key_to_index.keys())\n",
    "\n",
    "def charMapping(word_set):\n",
    "    final_word_set = set()\n",
    "    charMap = {}\n",
    "    for word in vocab:\n",
    "        if word in word_set:\n",
    "            charMap[word] = word\n",
    "            final_word_set.add(word)\n",
    "        elif word.lower() in word_set:\n",
    "            charMap[word.lower()] = word\n",
    "            final_word_set.add(word.lower())\n",
    "    word_set = final_word_set\n",
    "    return charMap\n",
    "\n",
    "def augment_wordset_with_OOV(word_set):\n",
    "    with open(r\"../oovMap.json\") as f:\n",
    "        dict = json.load(f)\n",
    "        # print(dict)\n",
    "        for key in dict:\n",
    "            words = dict[key]\n",
    "            for word in words:\n",
    "                word_set.add(word)\n",
    "            if key in word_set:\n",
    "                word_set.remove(key)\n",
    "\n",
    "def restrict_w2v(w2v, restricted_word_set, handle_oov = False):\n",
    "    if handle_oov:\n",
    "        augment_wordset_with_OOV(restricted_word_set)\n",
    "        charMap = charMapping(restricted_word_set)\n",
    "    \n",
    "    new_key_to_index = {} #given word, give index\n",
    "    new_index_to_key = {} #given index, give word\n",
    "    new_vectors = []\n",
    "    \n",
    "    new_key_to_index[\"</s>\"] = 0\n",
    "    new_index_to_key[0] = \"</s>\"\n",
    "    new_vectors.append([0] * 300)\n",
    "    \n",
    "    for word in restricted_word_set:\n",
    "        if word not in charMap:\n",
    "            continue\n",
    "        index = w2v.key_to_index[charMap[word]]\n",
    "        vec = w2v.vectors[index]\n",
    "        val = len(new_key_to_index)\n",
    "        new_key_to_index[word] = val\n",
    "        new_index_to_key[val] = word\n",
    "        new_vectors.append(vec)\n",
    "  \n",
    "    w2v.key_to_index = new_key_to_index\n",
    "    w2v.index_to_key = new_index_to_key\n",
    "    w2v.vectors = new_vectors\n",
    "\n",
    "restrict_w2v(word2vec_model, word_set, handle_oov=True)\n",
    "print(len(word2vec_model.vectors))\n",
    "# print(word2vec_model.key_to_index[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SentimentGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.2, bidirectional=True):\n",
    "        super(SentimentGRU, self).__init__()\n",
    "        self.embed_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.vectors), freeze=False, padding_idx=0)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Initialize the GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bias=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,  # Dropout is only applied if num_layers > 1\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        gru_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        self.fc = nn.Linear(gru_output_size, output_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embed_layer(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Compute the mask for the padded sequences\n",
    "        mask = (x.sum(dim=2) != 0).float()\n",
    "        lengths = mask.sum(dim=1).int()\n",
    "        \n",
    "        # Pack the padded sequence\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through the GRU\n",
    "        packed_out, hidden = self.gru(packed_x)\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # Apply mean pooling over valid parts only\n",
    "        out = (out * mask.unsqueeze(2)).sum(dim=1) / mask.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../oovMap.json\") as f:\n",
    "    oovMap = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, word2vec_model, max_length=100):\n",
    "        self.dataset = dataset\n",
    "        self.word2vec = word2vec_model\n",
    "        self.max_length = max_length  # Maximum sequence length for padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get text and label\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['label']\n",
    "        \n",
    "        # Convert text to embeddings\n",
    "        tokens = text # Assuming text is tokenized already\n",
    "        # embeddings = [self.word2vec[word] for word in tokens if word in self.word2vec]\n",
    "        # embeddings = [self.word2vec.key_to_index.get(word) for word in tokens if word in self.word2vec]\n",
    "        embeddings = []\n",
    "        # print(tokens)\n",
    "        for word in tokens:\n",
    "            # print(\"word: \", word)\n",
    "            if word in self.word2vec:\n",
    "                embeddings.append(self.word2vec.key_to_index.get(word))\n",
    "                continue\n",
    "            if word not in oovMap:\n",
    "                continue\n",
    "            for chunk in oovMap[word]:\n",
    "                # print(\"chunk:\" , chunk)\n",
    "                embeddings.append(self.word2vec.key_to_index.get(chunk))            \n",
    "        \n",
    "        # Truncate sequences - will pad later\n",
    "        if len(embeddings) > self.max_length:\n",
    "            embeddings = embeddings[:self.max_length]\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return self.__getitem__((idx + 1) % len(self.dataset))  # Skip empty sequences\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.int32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    '''Creates mini-batch tensors from the list of tuples (embeddings, labels, mask).'''\n",
    "    \n",
    "    # embedding_dim = batch[0][0].size(1)\n",
    "    \n",
    "    # Separate embeddings and labels\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # print(len(batch))     \n",
    "    # print(embeddings)\n",
    "    # Stack them into tensors\n",
    "    embeddings = pad_sequence(embeddings, batch_first=True) # (B, L, D)\n",
    "    # print(len(batch))     \n",
    "    # print([len(embeddings[i]) for i in range(len(embeddings))])\n",
    "    # Create the mask\n",
    "    mask = (embeddings != 0).float() # (B, L) - 1 if there is a word, 0 if it's a padding\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return embeddings, labels, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ranlo\\Downloads\\SC4002_NLP_Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# This is the training dataset\n",
    "path_to_train_set = r\"../tokenised_datasets/tokenised_train_dataset\"\n",
    "train_dataset = load_from_disk(path_to_train_set)\n",
    "train_data = SentimentDataset(train_dataset, word2vec_model)\n",
    "train_loader = DataLoader(train_data, batch_size=48, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Get the same for test and validation\n",
    "path_to_test_set = r\"../tokenised_datasets/tokenised_test_dataset\"\n",
    "test_dataset = load_from_disk(path_to_test_set)\n",
    "test_data = SentimentDataset(test_dataset, word2vec_model)\n",
    "test_loader = DataLoader(test_data, batch_size=48, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "path_to_val_set = r\"../tokenised_datasets/tokenised_validation_dataset\"\n",
    "val_dataset = load_from_disk(path_to_val_set)\n",
    "val_data = SentimentDataset(val_dataset, word2vec_model)\n",
    "val_loader = DataLoader(val_data, batch_size=48, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = word2vec_model.vector_size         # Dimension of Word2Vec embeddings\n",
    "hidden_size = 16        # Number of hidden units\n",
    "output_size = 1          # Output size (1 for binary classification)\n",
    "num_layers = 1           # Number of stacked RNN layers\n",
    "dropout = 0.60        # Dropout rate for regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device, criterion, val_losses=[], val_accuracies=[]):\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking for validation\n",
    "        for val_embeddings, val_labels, mask in val_loader:\n",
    "            val_embeddings, val_labels, mask = val_embeddings.to(device), val_labels.to(device), mask.to(device)\n",
    "            val_outputs = model(val_embeddings, mask).squeeze()\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            \n",
    "            val_loss += loss.item() * val_labels.size(0)  # Multiply by batch size for total loss\n",
    "            val_predictions = (torch.sigmoid(val_outputs) >= 0.5).float()  # Threshold at 0.5\n",
    "            val_correct_predictions += (val_predictions == val_labels).sum().item()\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / val_total_samples\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    \n",
    "    # Store validation metrics for plotting\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ranlo\\AppData\\Local\\Temp\\ipykernel_53964\\3960646852.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  self.embed_layer = nn.Embedding.from_pretrained(torch.FloatTensor(word2vec_model.vectors), freeze = False, padding_idx=0)\n",
      "c:\\Users\\ranlo\\Downloads\\SC4002_NLP_Project\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6931, Accuracy: 0.5039 Validation Accuracy: 0.5356\n",
      "Epoch 2/100, Loss: 0.6917, Accuracy: 0.5678 Validation Accuracy: 0.5929\n",
      "Epoch 3/100, Loss: 0.6899, Accuracy: 0.6188 Validation Accuracy: 0.6201\n",
      "Epoch 4/100, Loss: 0.6864, Accuracy: 0.6583 Validation Accuracy: 0.6370\n",
      "Epoch 5/100, Loss: 0.6763, Accuracy: 0.6803 Validation Accuracy: 0.6501\n",
      "Epoch 6/100, Loss: 0.6378, Accuracy: 0.7205 Validation Accuracy: 0.6735\n",
      "Epoch 7/100, Loss: 0.5833, Accuracy: 0.7543 Validation Accuracy: 0.6942\n",
      "Epoch 8/100, Loss: 0.5383, Accuracy: 0.7829 Validation Accuracy: 0.7129\n",
      "Epoch 9/100, Loss: 0.5017, Accuracy: 0.8020 Validation Accuracy: 0.7064\n",
      "Epoch 10/100, Loss: 0.4689, Accuracy: 0.8203 Validation Accuracy: 0.7176\n",
      "Epoch 11/100, Loss: 0.4419, Accuracy: 0.8333 Validation Accuracy: 0.7176\n",
      "Epoch 12/100, Loss: 0.4148, Accuracy: 0.8470 Validation Accuracy: 0.7186\n",
      "Epoch 13/100, Loss: 0.3915, Accuracy: 0.8586 Validation Accuracy: 0.7205\n",
      "Epoch 14/100, Loss: 0.3706, Accuracy: 0.8667 Validation Accuracy: 0.7195\n",
      "Epoch 15/100, Loss: 0.3507, Accuracy: 0.8747 Validation Accuracy: 0.7205\n",
      "Epoch 16/100, Loss: 0.3329, Accuracy: 0.8816 Validation Accuracy: 0.7251\n",
      "Epoch 17/100, Loss: 0.3179, Accuracy: 0.8907 Validation Accuracy: 0.7251\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# Binary Cross-Entropy Loss with Logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer with the model's parameters and a learning rate\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentLSTM(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for embeddings, labels, mask in train_loader:\n",
    "        # Move data to the same device as model (GPU if available)\n",
    "        embeddings, labels, mask = embeddings.to(device), labels.to(device), mask.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, mask).squeeze()\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the gradients\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(name, param.grad)\n",
    "        \n",
    "        # Accumulate the loss and accuracy\n",
    "        epoch_loss += loss.item() * labels.size(0)  # Multiply by batch size for total loss\n",
    "        predictions = (torch.sigmoid(outputs) >= 0.5).float()  # Threshold at 0.5\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    # Store metrics for plotting\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\", end=\" \")\n",
    "    validate_model(\n",
    "        model,\n",
    "        val_loader,\n",
    "        device,\n",
    "        criterion,\n",
    "        val_losses,\n",
    "        val_accuracies\n",
    "    )\n",
    "    # Early stopping: if the moving average validation accuracy has decreased, then stop\n",
    "    if epoch > 10 and val_accuracies[-1] < np.mean(val_accuracies[-10:-1]):\n",
    "        print(\"Early stopping\")\n",
    "        break \n",
    "\n",
    "# Plot the training and validation loss and accuracy over epochs\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss and accuracy over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# # Save the model\n",
    "torch.save(model.state_dict(), 'sentiment_rnn_1hl_{}_{}_with_OOV.pth'.format(hidden_size, str(learning_rate)[2:]))\n",
    "\n",
    "#load the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SentimentRNN(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "# model.load_state_dict(torch.load('sentiment_rnn_1hl_16_00005_with_OOV.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('reach')\n",
    "# This is the word2vec model\n",
    "# word2vec_model = KeyedVectors.load(r\"../word2vec-google-news-300.model\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "# print(len(test_loader))\n",
    "for i, x in enumerate(test_loader):\n",
    "    # print(i, end = \" \")\n",
    "    embeddings, labels, mask = x\n",
    "    # Move data to the same device as model\n",
    "    embeddings, labels, mask = embeddings.to(device), labels.to(device), mask.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(embeddings, mask).squeeze()\n",
    "    \n",
    "    # Store the actual and predicted labels\n",
    "    predictions = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "    test_predictions.extend(predictions.tolist())\n",
    "    test_targets.extend(labels.tolist())\n",
    "    \n",
    "# Calculate the test accuracy\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "test_accuracy = np.mean(test_predictions == test_targets)\n",
    "\n",
    "print(f\"testing accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
