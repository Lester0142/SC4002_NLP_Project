{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.2, activationfn='tanh',):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True, \n",
    "            dropout=dropout, \n",
    "            nonlinearity=activationfn, \n",
    "            bias=True\n",
    "            )\n",
    "    \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Inside your model's forward method:\n",
    "    def forward(self, x, mask, pooling='mean'):\n",
    "        # Pack the padded sequence\n",
    "        lengths = mask.sum(dim=1).int()  # Compute the lengths of the sequences (number of non-padded elements)\n",
    "        packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass through the RNN\n",
    "        packed_out, hidden = self.rnn(packed_x)\n",
    "        \n",
    "        if pooling == 'mean':\n",
    "        \n",
    "            # Unpack the sequence\n",
    "            out, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n",
    "            \n",
    "            # Perform mean pooling only over the valid (non-padded) parts\n",
    "            out = (out * mask.unsqueeze(2)).sum(dim=1) / mask.sum(dim=1, keepdim=True)  # Mean pooling\n",
    "            out = self.fc(out)  # Pass through the fully connected layer\n",
    "        \n",
    "        elif pooling == 'max':\n",
    "            \n",
    "            # Unpack the sequence to get all hidden states\n",
    "            out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "            \n",
    "            # Pass each hidden state through the fully connected layer to get sentiment scores\n",
    "            out = self.fc(out).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "\n",
    "            # Perform max pooling over the sentiment scores\n",
    "            out, _ = torch.max(out, dim=1)  # shape: (batch_size)\n",
    "            \n",
    "        elif pooling == 'last':\n",
    "            # Extract the final hidden state of the last layer\n",
    "            final_hidden_state = hidden[-1]  # Shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Pass the final hidden state through the fully connected layer\n",
    "            out = self.fc(final_hidden_state) # Shape: (batch_size, output_size)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Pooling mode '{pooling}' not supported.\")\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, word2vec_model, max_length=100):\n",
    "        self.dataset = dataset\n",
    "        self.word2vec = word2vec_model\n",
    "        self.max_length = max_length  # Maximum sequence length for padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get text and label\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['label']\n",
    "        \n",
    "        # Convert text to embeddings\n",
    "        tokens = text # Assuming text is tokenized already\n",
    "        embeddings = [self.word2vec[word] for word in tokens if word in self.word2vec]\n",
    "        \n",
    "        # Truncate sequences - will pad later\n",
    "        if len(embeddings) > self.max_length:\n",
    "            embeddings = embeddings[:self.max_length]\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return self.__getitem__((idx + 1) % len(self.dataset))  # Skip empty sequences\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        \n",
    "        return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    '''Creates mini-batch tensors from the list of tuples (embeddings, labels, mask).'''\n",
    "    \n",
    "    embedding_dim = batch[0][0].size(1)\n",
    "    \n",
    "    # Separate embeddings and labels\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "        \n",
    "    # print(\"BEFORE\")\n",
    "    # print(embeddings)\n",
    "    # Stack them into tensors\n",
    "    embeddings = pad_sequence(embeddings, batch_first=True) # (B, L, D)\n",
    "    # print(\"AFTER\")\n",
    "    # print(embeddings)\n",
    "    # Create the mask\n",
    "    mask = (embeddings.sum(dim=2) != 0).float() # (B, L) - 1 if there is a word, 0 if it's a padding\n",
    "    \n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return embeddings, labels, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhani\\Documents\\GitHub\\SC4002_NLP_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_from_disk\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Download the pre-trained Word2Vec model\n",
    "# NOTE TO DHANISH: PLEASE SAVE THIS DATASET SINCE IT TAKES A LONG TIME TO DOWNLOAD\n",
    "word2vec_model = KeyedVectors.load(r\"../word2vec-google-news-300.model\") \n",
    "\n",
    "# This is the training dataset\n",
    "path_to_train_set = r\"../tokenised_datasets/tokenised_train_dataset\"\n",
    "train_dataset = load_from_disk(path_to_train_set)\n",
    "train_data = SentimentDataset(train_dataset, word2vec_model)\n",
    "train_loader = DataLoader(train_data, batch_size=48, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Get the same for test and validation\n",
    "path_to_test_set = r\"../tokenised_datasets/tokenised_test_dataset\"\n",
    "test_dataset = load_from_disk(path_to_test_set)\n",
    "test_data = SentimentDataset(test_dataset, word2vec_model)\n",
    "test_loader = DataLoader(test_data, batch_size=48, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "path_to_val_set = r\"../tokenised_datasets/tokenised_validation_dataset\"\n",
    "val_dataset = load_from_disk(path_to_val_set)\n",
    "val_data = SentimentDataset(val_dataset, word2vec_model)\n",
    "val_loader = DataLoader(val_data, batch_size=48, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = word2vec_model.vector_size         # Dimension of Word2Vec embeddings\n",
    "hidden_size = 32        # Number of hidden units\n",
    "output_size = 1          # Output size (1 for binary classification)\n",
    "num_layers = 2           # Number of stacked RNN layers\n",
    "dropout = 0.60        # Dropout rate for regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device, criterion, val_losses=[], val_accuracies=[]):\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking for validation\n",
    "        for val_embeddings, val_labels, mask in val_loader:\n",
    "            val_embeddings, val_labels, mask = val_embeddings.to(device), val_labels.to(device), mask.to(device)\n",
    "            val_outputs = model(val_embeddings, mask).squeeze()\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            \n",
    "            val_loss += loss.item() * val_labels.size(0)  # Multiply by batch size for total loss\n",
    "            val_predictions = (torch.sigmoid(val_outputs) >= 0.5).float()  # Threshold at 0.5\n",
    "            val_correct_predictions += (val_predictions == val_labels).sum().item()\n",
    "            val_total_samples += val_labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / val_total_samples\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    \n",
    "    # Store validation metrics for plotting\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# Binary Cross-Entropy Loss with Logits\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizer with the model's parameters and a learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = SentimentRNN(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for embeddings, labels, mask in train_loader:\n",
    "        # Move data to the same device as model (GPU if available)\n",
    "        embeddings, labels, mask = embeddings.to(device), labels.to(device), mask.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(embeddings, mask).squeeze()\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the gradients\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(name, param.grad)\n",
    "        \n",
    "        # Accumulate the loss and accuracy\n",
    "        epoch_loss += loss.item() * labels.size(0)  # Multiply by batch size for total loss\n",
    "        predictions = (torch.sigmoid(outputs) >= 0.5).float()  # Threshold at 0.5\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    # Store metrics for plotting\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\", end=\" \")\n",
    "    validate_model(\n",
    "        model,\n",
    "        val_loader,\n",
    "        device,\n",
    "        criterion,\n",
    "        val_losses,\n",
    "        val_accuracies\n",
    "    )\n",
    "    # Early stopping: if the moving average validation accuracy has decreased, then stop\n",
    "    if epoch > 10 and val_accuracies[-1] < np.mean(val_accuracies[-10:-1]):\n",
    "        print(\"Early stopping\")\n",
    "        break \n",
    "        \n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'sentiment_rnn_2hl_64_055_0006.pth')\n",
    "\n",
    "# Plot the training and validation loss and accuracy over epochs\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss and accuracy over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training Accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhani\\AppData\\Local\\Temp\\ipykernel_10368\\3402085520.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('sentiment_rnn_2hl_32_0001.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'sentiment_rnn_2hl_{}_{}.pth'.format(hidden_size, str(learning_rate)[2:]))\n",
    "\n",
    "#load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SentimentRNN(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
    "model.load_state_dict(torch.load('sentiment_rnn_2hl_32_0001.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testidation Accuracy: 0.7533\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('reach')\n",
    "# This is the word2vec model\n",
    "word2vec_model = KeyedVectors.load(r\"../word2vec-google-news-300.model\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "# print(len(test_loader))\n",
    "for i, x in enumerate(test_loader):\n",
    "    # print(i, end = \" \")\n",
    "    embeddings, labels, mask = x\n",
    "    # Move data to the same device as model\n",
    "    embeddings, labels, mask = embeddings.to(device), labels.to(device), mask.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(embeddings, mask).squeeze()\n",
    "    \n",
    "    # Store the actual and predicted labels\n",
    "    predictions = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "    test_predictions.extend(predictions.tolist())\n",
    "    test_targets.extend(labels.tolist())\n",
    "    \n",
    "# Calculate the test accuracy\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "test_accuracy = np.mean(test_predictions == test_targets)\n",
    "\n",
    "print(f\"testidation Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
